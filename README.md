# ğŸš€ Analyse stratÃ©gique des tweets d'Elon Musk

## ğŸ’¬ Description

Ce projet permet d'**automatiser la collecte, l'analyse et la structuration des tweets** d'un compte Twitter (ici Elon Musk), afin d'identifier et classifier les informations stratÃ©giques ou financiÃ¨res susceptibles d'impacter ses entreprises.

Il combine :
- âœ… **TÃ©lÃ©chargement des tweets originaux** via une API
- âœ… **Annotation sÃ©mantique** par un modÃ¨le OpenAI (GPT)
- âœ… **Structuration JSON et enrichissement**
- âœ… **Visualisation interactive** avec Streamlit

---

## ğŸ—‚ï¸ Arborescence du projet
```
.
â”œâ”€â”€ app.py
â”œâ”€â”€ get_tweets.py
â”œâ”€â”€ Extraction_information.py
â”œâ”€â”€ Structuration.py
â”œâ”€â”€ .env
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```
---

## ğŸš€ Comment lancer le projet

### 1ï¸âƒ£ Installer les dÃ©pendances

pip install -r requirements.txt


### 2ï¸âƒ£ CrÃ©er un fichier `.env` Ã  la racine

OPENAI_API_KEY=ta_clÃ©_openai_ici
TWITTER_API_KEY=ta_clÃ©_twitter_ici


### 3ï¸âƒ£ Lancer l'application Streamlit

streamlit run app.py


### 4ï¸âƒ£ Cliquer sur **Â« RafraÃ®chir et analyser les tweets Â»** dans l'interface

Le tableau final des tweets analysÃ©s s'affiche directement, et tu peux Ã©galement tÃ©lÃ©charger le CSV final.


---

## âš™ï¸ Fonctionnement global (pipeline)

### 1ï¸âƒ£ RÃ©cupÃ©ration des tweets

- Via `get_tweets.py` â†’ fonction `get_all_tweets()`
- Filtrage pour ne garder que les **tweets originaux** (sans rÃ©ponses ni retweets)
- Par dÃ©faut, **5 ou 10 tweets** seulement pour **limiter les appels API**, rÃ©duire les coÃ»ts, et Ã©viter les restrictions

ğŸ“„ **RÃ©sultat sauvegardÃ© :** `data/elonmusk_tweets.csv`

---

### 2ï¸âƒ£ Annotation des tweets

- Via `extraction_information.py`
- Chaque tweet est analysÃ© par OpenAI pour estimer son **impact potentiel** sur :
  - Tesla
  - SpaceX
  - xAI
  - Neuralink
  - Twitter/X

ğŸ“„ **RÃ©sultat sauvegardÃ© :** `data/elonmusk_tweets_annotated.csv`

---

### 3ï¸âƒ£ Structuration et enrichissement

- Via `Structuration.py`
- Nettoyage du JSON, extraction des jugements, entreprises impactÃ©es et explications
- Construction d'un CSV final lisible

ğŸ“„ **RÃ©sultat sauvegardÃ© :** `data/elonmusk_tweets_final.csv`

---

## ğŸ” Gestion des clÃ©s API (sÃ©curitÃ©)

Pour protÃ©ger les clÃ©s d'API, elles sont **stockÃ©es dans un fichier `.env`** non versionnÃ©.

### Exemple de fichier `.env`

OPENAI_API_KEY = ta_clÃ©_openai_ici

TWITTER_API_KEY = ta_clÃ©_twitterapi.io

---

## ğŸ’¾ Pourquoi sauvegarder chaque Ã©tape en CSV ?

L'approche par Ã©tapes sauvegardÃ©es permet de limiter drastiquement les appels Ã  l'IA, qui sont coÃ»teux en tokens et peuvent gÃ©nÃ©rer rapidement des frais Ã©levÃ©s, car chaque Ã©tape produit un fichier CSV que l'on peut reprendre.

Ainsi, si l'on souhaite rejouer la pipeline (ou seulement une partie), il nâ€™est pas nÃ©cessaire de refaire inutilement les appels aux API.

Cette logique a aussi Ã©tÃ© pensÃ©e en prÃ©vision d'une automatisation future.

---

## ğŸ–¥ï¸ Interface Streamlit

- DÃ©finie dans **`app.py`**
- Pipeline exÃ©cutÃ©e **uniquement sur clic utilisateur**, afin dâ€™Ã©viter de dÃ©clencher inutilement des appels API coÃ»teux
- Tableau final affichÃ© directement dans l'interface (donnÃ©es finales issues du CSV final) : Mauvaise lisibilitÃ© actuel mais amÃ©riorable pas la suite si-besoin.

---

## ğŸ’¡ Choix techniques & modÃ¨le

ğŸ§  **ModÃ¨le OpenAI utilisÃ©** : `o1-mini` - Le moins couteux en Token

ğŸ’¬ **Prompt strict et "JSON only"**, afin de garantir une rÃ©ponse parsable facilement, sans texte parasite.  
